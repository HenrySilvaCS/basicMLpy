{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# basicMLpy Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module is composed of many different basic machine learning techniques, aimed at implementing simple yet effective supervised learning methods. These methods are comprised of regression and classification algorithms. So far, these algorithms can fit k-class classification and univariate regression problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import linalg \n",
    "#CLASSIFICATION#\n",
    "def probability_k1(row,parameters):\n",
    "\t\"\"\"\n",
    "\tCalculates Pr(G = 0|X = x).\n",
    "\tInputs:\n",
    "\t    row: array_like\n",
    "\t         input array(row vector), represents a row of the matrix of input points(usually represented by X).\n",
    "\t    parameters: array\n",
    "\t         input the column vector of predictors.\n",
    "\tReturns:\n",
    "\t    result: float\n",
    "\t          outputs Pr(G = 0|X = x).\n",
    "\t\"\"\"\n",
    "\tresult = np.exp(row @ parameters)/(1 + np.exp(row @ parameters))\n",
    "\treturn result \n",
    "def probability_vector(dataset,parameters):\n",
    "\t\"\"\"\n",
    "\tCalculates the vector of probabilities. Each element represents the probability of a given x belonging to class k = 0.\n",
    "\tInputs:\n",
    "\t    dataset: array\n",
    "\t         input array of input points, already expected to include the intercept. \n",
    "\t    parameters: array\n",
    "\t         input the column vector of predictors.\n",
    "\tReturns: \n",
    "\t    p: array\n",
    "\t         outputs the p vector of probabilities\n",
    "\t\"\"\"\n",
    "\tp = np.zeros((len(dataset),1))\n",
    "\tfor i in range(len(dataset)):\n",
    "\t\tp[i] = probability_k1(dataset[i,:],parameters)\n",
    "\treturn p \n",
    "def weight_matrix(dataset,parameters):\n",
    "\t\"\"\"\n",
    "\tCalculates the diagonal matrix of weights, defined by: W[i,i] = Pr(G = 0|X = x_i) * (1 - Pr(G = 0|X = x_i)).\n",
    "\tInputs:\n",
    "\t    dataset: array\n",
    "\t         input array of input points, already expected to include the intercept.\n",
    "\t    parameters: array\n",
    "\t         input the column vector of predictors.\n",
    "\tOutputs:\n",
    "\t    w: array\n",
    "\t         outputs a diagonal matrix NxN(N being the number of train samples).\n",
    "\t\"\"\"\n",
    "\tw = np.eye(len(dataset))\n",
    "\tfor i in range(len(dataset)):\n",
    "\t\tw[i,i] = probability_k1(dataset[i,:],parameters) * (1 - probability_k1(dataset[i,:],parameters))\n",
    "\treturn w \n",
    "def newton_step(dataset,y,n_iter):\n",
    "\t\"\"\"\n",
    "\tCalculates the newton step for a given array of input points and it's corresponding vector of output points.\n",
    "\tInputs:\n",
    "\t    dataset: array\n",
    "\t         input array of input points, already expected to include the intercept.\n",
    "\t    y: array\n",
    "\t         input array of output points, usually a column vector.\n",
    "\t\"\"\"\n",
    "\ttheta = np.zeros((np.size(dataset,1),1))\n",
    "\tfor i in range(n_iter):\n",
    "\t\tz = dataset @ theta + np.linalg.pinv(weight_matrix(dataset,theta)) @ (y - probability_vector(dataset,theta))\n",
    "\t\ttheta = np.linalg.pinv(dataset.T @ weight_matrix(dataset,theta) @ dataset) @ dataset.T @ weight_matrix(dataset,theta) @ z \n",
    "\treturn theta\n",
    "def binary_classification_default(x,y):\n",
    "\t\"\"\"\n",
    "\tFits a binary classification model on a given dataset of k = 2 classes.\n",
    "\tSplits the dataset(x,y) into training and validation sets, using the train_test_split function from sklearn. Default test_size = 0.2.\n",
    "\tInputs:\n",
    "\t    x: array\n",
    "\t        input array of input points to be used as training set, without the intercept(raw data).\n",
    "\t    y: array\n",
    "\t        input array of output points, usually a column vector with same number of rows as x.\n",
    "\tReturns:\n",
    "\t    theta: array\n",
    "\t        outputs array of predictors/parameters calculated by the algorithm.\n",
    "\t    accuracy: float\n",
    "\t        outputs the approximate percentual accuracy of the model, counting each misclassification on the validation set and calculating the final score.    \n",
    "\t\texp_loss: float\n",
    "            outputs the approximate exponential loss of the model on the validation set.\n",
    "        prediction_final: array\n",
    "            outputs the array of predictions over all inputs using the calculated parameters.    \n",
    "\t\"\"\"\n",
    "\tones = np.ones((len(x),1))\n",
    "\tx = np.hstack((ones,x))\n",
    "\tX_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size = 0.2, random_state=5)\n",
    "\tY_train = Y_train.reshape((len(Y_train),1))\n",
    "\ttheta = newton_step(X_train,Y_train,15)\n",
    "\tprediction = probability_vector(X_test,theta)\n",
    "\tp = np.round(prediction)\n",
    "\tcounter = 0\n",
    "\tfor i in range(len(prediction)):\n",
    "\t\tif np.absolute(p[i] - Y_test[i]) == 0:\n",
    "\t\t\tcounter = counter \n",
    "\t\telse:\n",
    "\t\t\tcounter += 1\n",
    "\taccuracy = np.round(((np.size(Y_test,0) - counter)/np.size(Y_test,0)) * 100)\n",
    "\texp_loss = 0\n",
    "\tfor i in range(len(prediction)):\n",
    "\t\texp_loss += np.exp(-1 * Y_test[i] * prediction[i])\n",
    "\t\tprediction_final = probability_vector(x,theta)\n",
    "\treturn theta, accuracy, np.round(exp_loss,2), np.round(prediction_final)\n",
    "def one_vs_all_default(x,y,k):\n",
    "\t\"\"\"\n",
    "\tFits a one-vs-all classification model on a given dataset of k > 2 classes.\n",
    "\tSplits the dataset(x,y) into training and validation sets, using the train_test_split function from sklearn. Default test_size = 0.2.\n",
    "\tInputs:\n",
    "\t    x: array\n",
    "\t        input array of input points to be used as training set, without the intercept(raw data).\n",
    "\t    y: array\n",
    "\t        input array of output points, usually a column vector with same number of rows as x.\n",
    "\tReturns:\n",
    "\t    theta: array\n",
    "\t        outputs array of predictors/parameters calculated by the algorithm.\n",
    "\t    accuracy: float\n",
    "\t        outputs the approximate percentual accuracy of the model, counting each misclassification on the validation set and calculating the final score.\n",
    "\t\texp_loss: float\n",
    "            outputs the approximate exponential loss of the model on the validation set.\n",
    "        result_final: array\n",
    "            outputs the array of predictions over all inputs using the calculated parameters.\n",
    "\t\"\"\"\n",
    "\tif k <= 2:\n",
    "\t\tprint(\"K must be bigger than two\")\n",
    "\t\treturn ValueError \n",
    "\telse:\n",
    "\t\tones = np.ones((np.size(x,0),1))\n",
    "\t\tx = np.hstack((ones,x))\n",
    "\t\tX_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size = 0.2, random_state=5)\n",
    "\t\tY_train = Y_train.reshape((np.size(Y_train,0),1))\n",
    "\t\tprobability_matrix = np.zeros((len(X_test),k))\n",
    "\t\tprob_index = np.zeros((len(x),k))\n",
    "\t\ttheta = np.zeros((np.size(X_test,1),k))\n",
    "\t\ttarget = np.zeros((len(Y_train),1))\n",
    "\t\tfor t in range(len(Y_train)):\n",
    "\t\t\ttarget[t] = Y_train[t]\n",
    "\t\tfor i in range(k):\n",
    "\t\t\tfor w in range(len(Y_train)):\n",
    "\t\t\t\tif Y_train[w] == i:\n",
    "\t\t\t\t\ttarget[w] = 1\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\ttarget[w] = 0\n",
    "\t\t\tparameters = newton_step(X_train,target,15)\n",
    "\t\t\ttheta[:,i] = parameters[:,0]\n",
    "\t\t\tprob = probability_vector(X_test,parameters)\n",
    "\t\t\tprob_final = probability_vector(x,parameters)\n",
    "\t\t\tprobability_matrix[:,i] = prob[:,0]\n",
    "\t\t\tprob_index[:,i] = prob_final[:,0]\n",
    "\t\tresult = np.zeros((len(probability_matrix),1))\n",
    "\t\tresult_final = np.zeros((len(x),1))\n",
    "\t\tresults_loss = np.zeros((len(probability_matrix),1))\n",
    "\t\tfor i in range(len(probability_matrix)):\n",
    "\t\t\tresults_loss[i,0] = probability_matrix[i,np.argmax(probability_matrix[i,:])]\n",
    "\t\tfor n in range(len(probability_matrix)):\n",
    "\t\t\tresult[n,0] = np.argmax(probability_matrix[n,:])\n",
    "\t\tfor i in range(len(prob_final)):\n",
    "\t\t\tresult_final[i,0] = np.argmax(prob_index[i,:])\n",
    "\t\tfor i in range(len(result)):\n",
    "\t\t\tcounter = 0\n",
    "\t\t\tif np.absolute(result[i] - Y_test[i]) == 0:\n",
    "\t\t\t\tcounter = counter \n",
    "\t\t\telse:\n",
    "\t\t\t\tcounter = counter + 1\n",
    "\t\taccuracy = np.round(((np.size(Y_test,0) - counter)/np.size(Y_test,0)) * 100)\n",
    "\t\texp_loss = 0\n",
    "\t\tfor i in range(len(result)):\n",
    "\t\t\texp_loss += np.exp(-1 * Y_test[i] * results_loss[i] )\n",
    "\t\treturn theta, accuracy, np.round(exp_loss,2), result_final\n",
    "def acc_and_loss(xtest,ytest,parameters,k):\n",
    "\t\"\"\"\n",
    "\tCalculates the accuracy and the huber loss of a given model w.r.t. the test dataset.\n",
    "    Inputs:\n",
    "        xtest: array\n",
    "            input array of all points that constitute the input test dataset.\n",
    "        ytest: array\n",
    "            input array of all points that constitute the output test dataset.\n",
    "        parameters: array\n",
    "            input array of parameters/predictors to be used in the prediction algorithm.\n",
    "        k: integer\n",
    "            input number of classes of the classification problem.\n",
    "    Returns:\n",
    "        accuracy: float\n",
    "            outputs the approximate percentual accuracy of the model w.r.t. the test set.\n",
    "        exp_loss\" float\n",
    "            outputs the approximate exponential loss of the model w.r.t. the test set.\n",
    "        \n",
    "\t\"\"\"\n",
    "\tytest = ytest.reshape((-1,1))\n",
    "\tif k == 1:\n",
    "\t\tprint(\"k must be >= 2\")\n",
    "\t\treturn ValueError \n",
    "\tif k == 2:\n",
    "\t\tones = np.ones((np.size(xtest,0),1))\n",
    "\t\txtest = np.hstack((ones,xtest))\n",
    "\t\tprediction = np.round(probability_vector(xtest,parameters))\n",
    "\t\tcounter = 0\n",
    "\t\tfor i in range(len(ytest)):\n",
    "\t\t\tif np.absolute(ytest[i] - prediction[i]) == 0:\n",
    "\t\t\t\tcounter = counter\n",
    "\t\t\telse:\n",
    "\t\t\t\tcounter += 1\n",
    "\t\taccuracy = ((len(ytest) - counter)/len(ytest)) * 100\n",
    "\t\texp_loss = 0\n",
    "\t\tprediction_loss = prediction\n",
    "\t\tfor i in range(len(ytest)):\n",
    "\t\t\texp_loss += np.exp(-1 * ytest[i] * prediction_loss[i])\n",
    "\t\treturn np.round(accuracy,2), np.round(exp_loss,2)\n",
    "\tif k >= 3:\n",
    "\t\tones = np.ones((np.size(xtest,0),1))\n",
    "\t\txtest = np.hstack((ones,xtest))\n",
    "\t\tprobability_matrix = np.zeros((len(ytest),k))\n",
    "\t\tprediction = np.zeros((len(ytest),1))\n",
    "\t\tfor j in range(k):\n",
    "\t\t\tprob = probability_vector(xtest,parameters[:,j])\n",
    "\t\t\tprobability_matrix[:,j] = prob[:,0]\n",
    "\t\tfor i in range(len(prediction)):\n",
    "\t\t\tprediction[i,0] = np.argmax(probability_matrix[i,:])\n",
    "\t\tresults_loss = np.zeros((len(probability_matrix),1))\n",
    "\t\tfor i in range(len(probability_matrix)):\n",
    "\t\t\tresults_loss[i,0] = probability_matrix[i,np.argmax(probability_matrix[i,:])]\n",
    "\t\tcounter = 0\n",
    "\t\tfor i in range(len(ytest)):\n",
    "\t\t\tif np.absolute(ytest[i] - prediction[i]) == 0:\n",
    "\t\t\t\tcounter = counter\n",
    "\t\t\telse:\n",
    "\t\t\t\tcounter += 1\n",
    "\t\taccuracy = ((len(ytest) - counter)/len(ytest)) * 100\n",
    "\t\texp_loss = 0\n",
    "\t\tfor i in range(len(ytest)):\n",
    "\t\t\texp_loss += np.exp(-1 * ytest[i] * results_loss[i] )\n",
    "\t\treturn np.round(accuracy,2), np.round(exp_loss,2)\n",
    "class classification_fit:\n",
    "\t\"\"\"\n",
    "\tClass of two different types of classification fits for a given dataset.\n",
    "\tExecutes a given model based on user input.\n",
    "\t\tInputs:\n",
    "\t\t\tx: array\n",
    "\t\t\t\tinput array of input points to be used as training set, without the intercept(raw data).\n",
    "\t\t\ty: array\n",
    "\t\t\t\tinput array of output points, usually a column vector with same number of rows as x.\n",
    "\t\t\tk: int\n",
    "\t\t\t\tinput the number k of classes associated with the dataset \n",
    "\t\tReturns:\n",
    "\t\t\tself.bt: array\n",
    "\t\t\t\toutputs the array of parameters/predictors calculated by the binary model.\n",
    "\t\t\tself.ba: float\n",
    "\t\t\t\toutputs the accuracy of the binary model.\n",
    "\t\t\tself.bel: float\n",
    "\t\t\t    outputs the approximate exponential loss of the model on the validation set.\n",
    "\t\t\tself.bpredict: array\n",
    "\t\t\t    outputs the array of predictions over all inputs using the calculated parameters.\n",
    "\t\t\tself.kt: array \n",
    "\t\t\t\toutputs the array of parameters/predictors calculated by the one-vs-all model.\n",
    "\t\t\tself.ka: float\n",
    "\t\t\t\toutputs the accuracy of the one-vs-all model. \n",
    "\t\t\tself.kel: float\n",
    "\t\t\t    outputs the approximate exponential loss of the model on the validation set.\n",
    "\t\t\tself.kpredict:\n",
    "\t\t\t    outputs the array of predictions over all inputs using the calculated parameters.\n",
    "\t\"\"\"\n",
    "\tdef __init__(self,x,y,k):\n",
    "\t\tif k == 2:\n",
    "\t\t\tself.bt = binary_classification_default(x,y)[0]\n",
    "\t\t\tself.ba = binary_classification_default(x,y)[1]\n",
    "\t\t\tself.bel = binary_classification_default(x,y)[2]\n",
    "\t\t\tself.bpredict = binary_classification_default(x,y)[3]\n",
    "\t\telse:\n",
    "\t\t\tself.kt = one_vs_all_default(x,y,k)[0]\n",
    "\t\t\tself.ka = one_vs_all_default(x,y,k)[1]\n",
    "\t\t\tself.kel = one_vs_all_default(x,y,k)[2]\n",
    "\t\t\tself.kpredict = one_vs_all_default(x,y,k)[3]\n",
    "#REGRESSION#\n",
    "def regressionQR(x,y):\n",
    "\t\"\"\"\n",
    "\tCalculates the predictors for a linear regression model, using QR decomposition.\n",
    "\tInputs:\n",
    "\t    x: array\n",
    "\t        input array of input points, with the intercept.\n",
    "\t    y: array\n",
    "\t        input array of output points, usually a column vector with same number of rows as x.\n",
    "\tReturns:\n",
    "\t    theta: array\n",
    "\t        outputs the array of predictors for the regression model.\n",
    "\t\"\"\"\n",
    "\tq, r = np.linalg.qr(x) \n",
    "\tb = q.T @ y\n",
    "\ttheta = linalg.solve_triangular(r,b)\n",
    "\treturn theta\n",
    "def calculate_error(x,y,parameters):\n",
    "\t\"\"\"\n",
    "\tCalculates the squared error of a given model.\n",
    "\tInputs:\n",
    "\t    x: array\n",
    "\t        input array of input points, with the intercept.\n",
    "\t    y: array\n",
    "\t        input array of output points, usually a column vector with same number of rows as x.\n",
    "\t    parameters: array\n",
    "\t        input the column vector of predictors.\n",
    "\tReturns:\n",
    "\t    errors: array\n",
    "\t        outputs a column vector of squared errors.\n",
    "\t    errors_sum: float\n",
    "\t        outputs the sum of the errors vector.\n",
    "\t\"\"\"\n",
    "\tprediction = (x @ parameters).reshape((np.size(x,0)),1)\n",
    "\terrors = np.square(prediction - y)\n",
    "\terrors_sum = sum(errors)\n",
    "\treturn errors, errors_sum\n",
    "def linear_regression(x,y):\n",
    "\t\"\"\"\n",
    "\tFits a linear regression model on a given dataset.\n",
    "\tInputs:\n",
    "\t\tx: array\n",
    "\t\t\tinput array of input points to be used as training set, without the intercept(raw data).\n",
    "\t\ty: array\n",
    "\t\t\tinput array of output points, usually a column vector.\n",
    "\tReturns:\n",
    "\t\ttheta: array\n",
    "\t\t\toutputs the array of predictors for the regression model.\n",
    "\t\tmse: float\n",
    "\t\t\toutputs the approximate Mean Squared Error found by the regression model on the validation set. mse is rounded up to 2 decimal cases.\n",
    "\t\tprediction: array\n",
    "\t\t\toutputs the array of predictions over all inputs using the calculated parameters.\n",
    "\t\thuber_loss: float\n",
    "\t\t\toutputs the approximate huber loss for the validation set.\n",
    "\t\"\"\"\n",
    "\tones = np.ones((len(x),1))\n",
    "\tx = np.hstack((ones,x))\n",
    "\tX_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size = 0.2, random_state=5)\n",
    "\tX_test = X_test.reshape((-1,X_train.shape[1]))\n",
    "\tY_test = Y_test.reshape((-1,1))\n",
    "\ttheta = regressionQR(X_train,Y_train)\n",
    "\terror = calculate_error(X_test,Y_test,theta)[1]\n",
    "\tmse = error/np.size(X_test,0)\n",
    "\tprediction = x @ theta\n",
    "\thuber_loss = 0\n",
    "\tpred = X_test @ theta\n",
    "\tdelta = np.quantile(Y_test - pred,0.5)\n",
    "\tfor i in range(len(X_test)):\n",
    "\t\tif np.absolute(Y_test[i] - pred[i] ) <= delta:\n",
    "\t\t\thuber_loss += np.square(Y_test[i] - pred[i])\n",
    "\t\telse:\n",
    "\t\t\thuber_loss += 2 * delta * np.absolute(Y_test[i] - pred[i]) - delta**2\n",
    "\treturn theta, np.round(mse,2), prediction, np.round(huber_loss,2)\n",
    "def basis_expansion(x,y,btype):\n",
    "\t\"\"\"\n",
    "\tExecutes a basis expansion on the array of inputs and then fits a linear regression model on the dataset.\n",
    "\tInputs:\n",
    "\t\tx: array\n",
    "\t\t\tinput array of input points to be used as training set, without the intercept(raw data).\n",
    "\t\ty: array\n",
    "\t\t\tinput array of output points, usually a column vector with same number of rows as x.\n",
    "\t\tbtype: string\n",
    "\t\t\tinput string that identifies the type of basis expansion; btype can be: None, sqrt, poly.\n",
    "\tReturns:\n",
    "\t\ttheta: array\n",
    "\t\t\toutputs the array of predictors for the regression model.\n",
    "\t\tmse: float\n",
    "\t\t\toutputs the approximate Mean Squared Error found by the regression model for the validation set. mse is rounded up to 2 decimal cases.\n",
    "\t\tprediction: array\n",
    "\t\t\toutputs the array of predictions over all inputs using the calculated parameters.\n",
    "\t\thuber_loss: float\n",
    "\t\t\toutputs the approximate huber loss for the validation set.\n",
    "    *REMEMBER TO APPLY THE SAME BASIS EXPANSION ON THE TEST SET.\n",
    "\t\"\"\"\n",
    "\tones = np.ones((np.size(x,0),1))\n",
    "\tx = np.hstack((ones,x))\n",
    "\tx_new = np.zeros((len(x),np.size(x,1)))\n",
    "\tfor i in range(len(x)):\n",
    "\t\tx_new[i,:] = x[i,:]\n",
    "\tif btype == 'sqrt':\n",
    "\t\tX_train, X_test, Y_train, Y_test = train_test_split(x_new, y, test_size = 0.2, random_state=5)\n",
    "\t\tX_test = X_test.reshape((-1,np.size(X_train,1)))\n",
    "\t\tY_test = Y_test.reshape((-1,1))\n",
    "\t\tfor i in range(np.size(x,1)):\n",
    "\t\t\tX_train[:,i] = np.sqrt(X_train[:,i])\n",
    "\t\t\tX_test[:,i] = np.sqrt(X_test[:,i])\n",
    "\t\ttheta = regressionQR(X_train,Y_train)\n",
    "\t\terror = calculate_error(X_test,Y_test,theta)[1]\n",
    "\t\tmse = error/np.size(X_test,0)\n",
    "\t\tprediction = x @ theta\n",
    "\t\tpred = X_test @ theta\n",
    "\t\thuber_loss = 0\n",
    "\t\tdelta = np.quantile(Y_test - pred,0.5)\n",
    "\t\tfor i in range(len(X_test)):\n",
    "\t\t\tif np.absolute(Y_test[i] - pred[i] ) <= delta:\n",
    "\t\t\t\thuber_loss += np.square(Y_test[i] - pred[i])\n",
    "\t\t\telse:\n",
    "\t\t\t\thuber_loss += 2 * delta * np.absolute(Y_test[i] - pred[i]) - delta**2\n",
    "\t\treturn theta,np.round(mse,2), prediction, np.round(huber_loss,2)\n",
    "\tif btype == 'poly':\n",
    "\t\tX_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size = 0.2, random_state=5)\n",
    "\t\tX_test = X_test.reshape((-1,np.size(X_train,1)))\n",
    "\t\tY_test = Y_test.reshape((-1,1))\n",
    "\t\tfor i in range(np.size(x,1)):\n",
    "\t\t\tX_train[:,i] = (X_train[:,i] **(i)) \n",
    "\t\t\tX_test[:,i] = (X_test[:,i] **(i))\n",
    "\t\ttheta = regressionQR(X_train,Y_train)\n",
    "\t\terror = calculate_error(X_test,Y_test,theta)[1]\n",
    "\t\tmse = error/np.size(X_test,0)\n",
    "\t\tprediction = x @ theta\n",
    "\t\tpred = X_test @ theta\n",
    "\t\thuber_loss = 0\n",
    "\t\tdelta = 0.5\n",
    "\t\tfor i in range(len(X_test)):\n",
    "\t\t\tif np.absolute(Y_test[i] - pred[i] ) <= delta:\n",
    "\t\t\t\thuber_loss += np.square(Y_test[i] - pred[i])\n",
    "\t\t\telse:\n",
    "\t\t\t\thuber_loss += 2 * delta * np.absolute(Y_test[i] - pred[i]) - delta**2\n",
    "\t\treturn theta,np.round(mse,2), prediction, np.round(huber_loss,2)\n",
    "\telse:\n",
    "\t\tprint(\"Insert a valid expansion type\")\n",
    "\t\treturn ValueError \n",
    "def ridge_regression(x,y,const):\n",
    "\t\"\"\"\n",
    "\tFits a ridge linear regression model on a given dataset.\n",
    "\tInputs:\n",
    "\t\tx: array\n",
    "\t\t\tinput array of input points to be used as training set, with the intercept.\n",
    "\t\ty: array\n",
    "\t\t\tinput array of output points, usually a column vector with same number of rows as x.\n",
    "\t\tconst: float\n",
    "\t\t\tinput the value for the penalizing constant(lambda) used by the ridge algorithm.\n",
    "\tReturns:\n",
    "\t\ttheta: array\n",
    "\t\t\toutputs the array of predictors for the regression model.\n",
    "\t\tmse: float\n",
    "\t\t\toutputs the approximate Mean Squared Error found by the regression model. mse is rounded up to 2 decimal cases.\n",
    "\t\tprediction: array\n",
    "\t\t\toutputs the array of predictions over all inputs using the calculated parameters.\n",
    "\t\thuber_loss: float\n",
    "\t\t\toutputs the approximate huber loss for the validation set.\n",
    "\t\"\"\"\n",
    "\tones = np.ones((np.size(x,0),1))\n",
    "\tx = np.hstack((ones,x))\n",
    "\tX_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size = 0.2, random_state=5)\n",
    "\tX_test = X_test.reshape((-1,np.size(X_train,1)))\n",
    "\tY_test = Y_test.reshape((-1,1))\n",
    "\tidentity = np.eye(np.size(X_train,1))\n",
    "\ttheta = np.linalg.inv(X_train.T @ X_train + const * identity) @ X_train.T @ Y_train \n",
    "\terror = calculate_error(X_test,Y_test,theta)[1]\n",
    "\tmse = error/np.size(X_test,0)\n",
    "\tprediction = x @ theta\n",
    "\tpred = X_test @ theta\n",
    "\thuber_loss = 0\n",
    "\tdelta = np.quantile(Y_test - pred,0.5)\n",
    "\tfor i in range(len(X_test)):\n",
    "\t\tif np.absolute(Y_test[i] - pred[i] ) <= delta:\n",
    "\t\t\thuber_loss += np.square(Y_test[i] - pred[i])\n",
    "\t\telse:\n",
    "\t\t\thuber_loss += 2 * delta * np.absolute(Y_test[i] - pred[i]) - delta**2\n",
    "\treturn theta, np.round(mse,2), prediction, np.round(huber_loss)\n",
    "def mse_and_huber(xtest,ytest,parameters):\n",
    "    \"\"\"\n",
    "    Calculates the Mean Squared Error and the Huber Loss of a given model w.r.t. the test dataset.\n",
    "    Inputs:\n",
    "        xtest: array\n",
    "            input array of all points that constitute the input test dataset, without the intercept(raw data).\n",
    "        ytest: array\n",
    "            input array of all points that constitute the output test dataset.\n",
    "        parameters: array\n",
    "            input array of parameters/predictors to be used in the prediction algorithm.\n",
    "    Returns:\n",
    "        mse: float\n",
    "            outputs the approximate Mean Squared Error w.r.t. the test set.\n",
    "        huber_loss: float\n",
    "            outputs the approximate Huber Loss w.r.t. the test set.\n",
    "            \n",
    "        \n",
    "\"\"\"\n",
    "    ones = np.ones((len(xtest),1))\n",
    "    xtest = np.hstack((ones,xtest))\n",
    "    prediction = xtest @ parameters\n",
    "    error = sum(np.square(ytest - prediction))\n",
    "    mse = error/len(ytest)\n",
    "    huber_loss = 0\n",
    "    delta = 0.5\n",
    "    for i in range(len(xtest)):\n",
    "        if np.absolute(ytest[i] - prediction[i] ) <= delta:\n",
    "            huber_loss += np.square(ytest[i] - prediction[i])\n",
    "        else:\n",
    "            huber_loss += 2 * delta * np.absolute(ytest[i] - prediction[i]) - delta**2\n",
    "    return np.round(mse,2), np.round(huber_loss,2)\n",
    "class regression_fit:\n",
    "\t\"\"\"\n",
    "\tClass of three different regression models.\n",
    "\tExecutes a given model based on user input.\n",
    "\tInputs:\n",
    "\t\tx: array\n",
    "\t\t\tinput array of input points, without the intercept(raw data).\n",
    "\t\ty: array\n",
    "\t\t\tinput array of output points, usually a column vector.\n",
    "\t\tbtype: string\n",
    "\t\t\tinput string that identifies the type of basis expansion; btype can be: None, sqrt, poly.\n",
    "\t\treg_lambda: float\n",
    "\t\t\tinput string specifies the regularization constant to be used in the ridge regresssion; input 'None' if not using it.\n",
    "\tReturns:\n",
    "\t\tself.lt: array\n",
    "\t\t\toutputs the array of parameters/predictors calculated by the standard linear model.\n",
    "\t\tself.le: float\n",
    "\t\t\toutputs the approximate Mean Squared Error calculated by the standard linear model.\n",
    "\t\tself.lhl: float\n",
    "\t\t\toutputs the approximate huber loss for the validation set.\n",
    "\t\tself.lpredict: array\n",
    "\t\t\toutputs the array of predictions over all inputs using the calculated parameters.\n",
    "\t\tself.rt: array\n",
    "\t\t\toutputs the array of parameters/predictors calculated by the ridge linear model.\n",
    "\t\tself.re: float\n",
    "\t\t\toutputs the approximate Mean Squared Error calculated by the ridge linear model.\n",
    "\t\tself.rhl: float\n",
    "\t\t\toutputs the approximate huber loss for the validation set.\n",
    "\t\tself.rpredict: array\n",
    "\t\t\toutputs the array of predictions over all inputs using the calculated parameters.\n",
    "\t\tself.bet: array\n",
    "\t\t\toutputs the array of parameters/predictors calculated by the basis expansion model.\n",
    "\t\tself.bee: float\n",
    "\t\t\toutputs the approximate Mean Squared Error calculated by the basis expansion model.\n",
    "\t\tself.behl: float\n",
    "\t\t\toutputs the approximate huber loss for the validation set.\n",
    "\t\tself.bepredict: array\n",
    "\t\t\toutputs the array of predictions over all inputs using the calculated parameters.\n",
    "    \"\"\"\n",
    "\tdef __init__(self,x,y,btype,reg_lambda):\n",
    "\t\tif btype == None:\n",
    "\t\t\tif reg_lambda == None:            \n",
    "\t\t\t\tself.lt = linear_regression(x,y)[0]\n",
    "\t\t\t\tself.le = linear_regression(x,y)[1]\n",
    "\t\t\t\tself.lpredict = linear_regression(x,y)[2]\n",
    "\t\t\t\tself.lhl = linear_regression(x,y)[3]\n",
    "\t\t\telse:\n",
    "\t\t\t\tself.rt = ridge_regression(x,y,reg_lambda)[0]\n",
    "\t\t\t\tself.re = ridge_regression(x,y,reg_lambda)[1]\n",
    "\t\t\t\tself.rpredict = ridge_regression(x,y,reg_lambda)[2]\n",
    "\t\t\t\tself.rhl = ridge_regression(x,y,reg_lambda)[3]\n",
    "\t\telse:\n",
    "\t\t\tself.bet = basis_expansion(x,y,btype)[0]\n",
    "\t\t\tself.bee = basis_expansion(x,y,btype)[1]\n",
    "\t\t\tself.bepredict = basis_expansion(x,y,btype)[2]\n",
    "\t\t\tself.behl = basis_expansion(x,y,btype)[3]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we'll see the actual performance of the algorithm for many different famous datasets available at scikitlearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import basicMLpy as bp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Wine recognition dataset, consisting of a classification problem with k = 3 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassifications on the training set: 0\n",
      "The predicted accuracy of the model is: 100.0%\n",
      "The predicted exponential loss of the model is: [16.63]\n",
      "The actual accuracy of the model is: 97.22%\n",
      "The actual exponential loss of the model is: [23.02]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "X,y = load_wine(return_X_y=True)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.2, random_state=5)\n",
    "result = bp.classification_fit(X_train,Y_train,3)\n",
    "counter = 0\n",
    "for i in range(len(Y_train)):\n",
    "    if result.kpredict[i] == Y_train[i]:\n",
    "        counter = counter\n",
    "    else:\n",
    "        counter += 1\n",
    "print(f\"Misclassifications on the training set: {counter}\")\n",
    "print(f\"The predicted accuracy of the model is: {result.ka}%\")\n",
    "print(f\"The predicted exponential loss of the model is: {result.kel}\")\n",
    "print(f\"The actual accuracy of the model is: {bp.acc_and_loss(X_test,Y_test,result.kt,3)[0]}%\")\n",
    "print(f\"The actual exponential loss of the model is: {bp.acc_and_loss(X_test,Y_test,result.kt,3)[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Iris plants dataset, consisting of a classification problem with k = 3 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassifications on the training set: 0\n",
      "The predicted accuracy of the model is: 100.0%\n",
      "The predicted exponential loss of the model is: [14.29]\n",
      "The actual accuracy of the model is: 93.33%\n",
      "The actual exponential loss of the model is: [16.82]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "X,y = load_iris(return_X_y=True)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.2, random_state=5)\n",
    "result = bp.classification_fit(X_train,Y_train,3)\n",
    "counter = 0\n",
    "for i in range(len(Y_train)):\n",
    "    if result.kpredict[i] == Y_train[i]:\n",
    "        counter = counter\n",
    "    else:\n",
    "        counter += 1\n",
    "print(f\"Misclassifications on the training set: {counter}\")\n",
    "print(f\"The predicted accuracy of the model is: {result.ka}%\")\n",
    "print(f\"The predicted exponential loss of the model is: {result.kel}\")\n",
    "print(f\"The actual accuracy of the model is: {bp.acc_and_loss(X_test,Y_test,result.kt,3)[0]}%\")\n",
    "print(f\"The actual exponential loss of the model is: {bp.acc_and_loss(X_test,Y_test,result.kt,3)[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Boston house prices dataset, consisting of a regression problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying a regular linear regression to the dataset\n",
      "The predicted MSE of the model is: [26.82]\n",
      "The predicted huber loss of the model is: [-206.18]\n",
      "The actual MSE of the model is: 21.46\n",
      "The actual huber loss of the model is: 302.88\n",
      "########################\n",
      "Applying a ridge regression to the dataset\n",
      "The predicted MSE of the model is: [26.58]\n",
      "The predicted huber loss of the model is: [-192.]\n",
      "The actual MSE of the model is: 21.46\n",
      "The actual huber loss of the model is: 298.09\n",
      "########################\n",
      "Applying a square root basis expansion to the dataset\n",
      "The predicted MSE of the model is: [36.73]\n",
      "The predicted huber loss of the model is: [281.79]\n",
      "The actual MSE of the model is: 75.42\n",
      "The actual huber loss of the model is: 591.05\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "X,y = load_boston(return_X_y=True)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.2, random_state=5)\n",
    "result = regression_fit(X_train,Y_train,btype = None,reg_lambda = None)\n",
    "print(\"Applying a regular linear regression to the dataset\")\n",
    "print(f\"The predicted MSE of the model is: {result.le}\")\n",
    "print(f\"The predicted huber loss of the model is: {result.lhl}\")\n",
    "print(f\"The actual MSE of the model is: {mse_and_huber(X_test,Y_test,result.lt)[0]}\")\n",
    "print(f\"The actual huber loss of the model is: {mse_and_huber(X_test,Y_test,result.lt)[1]}\")\n",
    "print(\"########################\")\n",
    "result2 = regression_fit(X_train,Y_train,btype = None,reg_lambda = 0.1)\n",
    "print(\"Applying a ridge regression to the dataset\")\n",
    "print(f\"The predicted MSE of the model is: {result2.re}\")\n",
    "print(f\"The predicted huber loss of the model is: {result2.rhl}\")\n",
    "print(f\"The actual MSE of the model is: {mse_and_huber(X_test,Y_test,result2.rt)[0]}\")\n",
    "print(f\"The actual huber loss of the model is: {mse_and_huber(X_test,Y_test,result2.rt)[1]}\")\n",
    "print(\"########################\")\n",
    "result3 = regression_fit(X_train,Y_train,btype = 'poly',reg_lambda = None)\n",
    "for i in range(np.size(X_test,1)):\n",
    "    X_test[:,i] = np.power(X_test[:,i],i) \n",
    "print(\"Applying a square root basis expansion to the dataset\")\n",
    "print(f\"The predicted MSE of the model is: {result3.bee}\")\n",
    "print(f\"The predicted huber loss of the model is: {result3.behl}\")\n",
    "print(f\"The actual MSE of the model is: {mse_and_huber(X_test,Y_test,result3.bet)[0]}\")\n",
    "print(f\"The actual huber loss of the model is: {mse_and_huber(X_test,Y_test,result3.bet)[1]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Wisconsin breast cancer dataset, consisting of a classification problem with k = 2 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassifications on the training set: 1\n",
      "The predicted accuracy of the model is: 99.0%\n",
      "The predicted exponential loss of the model is: [52.44]\n",
      "The actual accuracy of the model is: 96.49%\n",
      "The actual exponential loss of the model is: [72.91]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "X,y = load_breast_cancer(return_X_y=True)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.2, random_state=5)\n",
    "result = bp.classification_fit(X_train,Y_train,2)\n",
    "counter = 0\n",
    "for i in range(len(Y_train)):\n",
    "    if result.bpredict[i] == Y_train[i]:\n",
    "        counter = counter\n",
    "    else:\n",
    "        counter += 1\n",
    "print(f\"Misclassifications on the training set: {counter}\")\n",
    "print(f\"The predicted accuracy of the model is: {result.ba}%\")\n",
    "print(f\"The predicted exponential loss of the model is: {result.bel}\")\n",
    "print(f\"The actual accuracy of the model is: {bp.acc_and_loss(X_test,Y_test,result.bt,2)[0]}%\")\n",
    "print(f\"The actual exponential loss of the model is: {bp.acc_and_loss(X_test,Y_test,result.bt,2)[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the module performs at its best for classification problems. For regression problems, I recommend you trying to filter and work on the data before using the module, e.g. by deleting outliers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
