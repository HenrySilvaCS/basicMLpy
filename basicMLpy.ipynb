{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# basicMLpy Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module is composed of many different basic machine learning techniques, aimed at implementing simple yet effective supervised learning methods. These methods are comprised of regression and classification algorithms, that will fit pretty much any standard dataset, with varying degrees of accuracy of course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import linalg \n",
    "#CLASSIFICATION#\n",
    "def probability_k1(row,parameters):\n",
    "\t\"\"\"\n",
    "\tCalculates Pr(G = 0|X = x).\n",
    "\tInputs:\n",
    "\t    row: array_like\n",
    "\t         input array(row vector), represents a row of the matrix of input points(usually represented by X).\n",
    "\t    parameters: array\n",
    "\t         input the column vector of predictors.\n",
    "\tReturns:\n",
    "\t    result: float\n",
    "\t          outputs Pr(G = 0|X = x).\n",
    "\t\"\"\"\n",
    "\tresult = np.exp(row @ parameters)/(1 + np.exp(row @ parameters))\n",
    "\treturn result \n",
    "def probability_vector(dataset,parameters):\n",
    "\t\"\"\"\n",
    "\tCalculates the vector of probabilities. Each element represents the probability of a given x belonging to class k = 0.\n",
    "\tInputs:\n",
    "\t    dataset: array\n",
    "\t         input array of input points, already expected to include the intercept. \n",
    "\t    parameters: array\n",
    "\t         input the column vector of predictors.\n",
    "\tReturns: \n",
    "\t    p: array\n",
    "\t         outputs the p vector of probabilities\n",
    "\t\"\"\"\n",
    "\tp = np.zeros((np.size(dataset,0),1))\n",
    "\tfor i in range(np.size(dataset,0)):\n",
    "\t\tp[i] = probability_k1(dataset[i,:],parameters)\n",
    "\treturn p \n",
    "def weight_matrix(dataset,parameters):\n",
    "\t\"\"\"\n",
    "\tCalculates the diagonal matrix of weights, defined by: W[i,i] = Pr(G = 0|X = x_i) * (1 - Pr(G = 0|X = x_i)).\n",
    "\tInputs:\n",
    "\t    dataset: array\n",
    "\t         input array of input points, already expected to include the intercept.\n",
    "\t    parameters: array\n",
    "\t         input the column vector of predictors.\n",
    "\tOutputs:\n",
    "\t    w: array\n",
    "\t         outputs a diagonal matrix NxN(N being the number of train samples).\n",
    "\t\"\"\"\n",
    "\tw = np.eye((np.size(dataset,0)))\n",
    "\tfor i in range(np.size(dataset,0)):\n",
    "\t\tw[i,i] = probability_k1(dataset[i,:],parameters) * (1 - probability_k1(dataset[i,:],parameters))\n",
    "\treturn w \n",
    "def newton_step(dataset,y,n_iter):\n",
    "\t\"\"\"\n",
    "\tCalculates the newton step for a given array of input points and it's corresponding vector of output points.\n",
    "\tInputs:\n",
    "\t    dataset: array\n",
    "\t         input array of input points, already expected to include the intercept.\n",
    "\t    y: array\n",
    "\t         input array of output points, usually a column vector.\n",
    "\t\"\"\"\n",
    "\ttheta = np.zeros((np.size(dataset,1),1))\n",
    "\tfor i in range(n_iter):\n",
    "\t\tz = dataset @ theta + np.linalg.pinv(weight_matrix(dataset,theta)) @ (y - probability_vector(dataset,theta))\n",
    "\t\ttheta = np.linalg.pinv(dataset.T @ weight_matrix(dataset,theta) @ dataset) @ dataset.T @ weight_matrix(dataset,theta) @ z \n",
    "\treturn theta\n",
    "def binary_classification_default(x,y):\n",
    "\t\"\"\"\n",
    "\tFits a binary classification model on a given dataset of k = 2 classes.\n",
    "\tSplits the dataset(x,y) into training and test sets, using the train_test_split function from sklearn. Default test_size = 0.2.\n",
    "\tInputs:\n",
    "\t    x: array\n",
    "\t        input array of input points, without the intercept(raw data).\n",
    "\t    y: array\n",
    "\t        input array of output points, usually a column vector.\n",
    "\tReturns:\n",
    "\t    theta: array\n",
    "\t        outputs array of predictors/parameters calculated by the algorithm.\n",
    "\t    accuracy: float\n",
    "\t        outputs the approximate percentual accuracy of the model, counting each misclassification and calculating the final score.    \n",
    "\t\"\"\"\n",
    "\tones = np.ones((np.size(x,0),1))\n",
    "\tx = np.hstack((ones,x))\n",
    "\tX_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size = 0.2, random_state=5)\n",
    "\ttheta = newton_step(X_train,Y_train,15)\n",
    "\tprediction = probability_vector(X_test,theta)\n",
    "\tp = np.round(prediction)\n",
    "\tcounter = 0\n",
    "\tfor i in range(np.size(prediction,0)):\n",
    "\t\tif np.absolute(p[i] - Y_test[i]) == 0:\n",
    "\t\t\tcounter = counter \n",
    "\t\telse:\n",
    "\t\t\tcounter += 1\n",
    "\taccuracy = np.round(((np.size(Y_test,0) - counter)/np.size(Y_test,0)) * 100)\n",
    "\treturn theta, accuracy\n",
    "def one_vs_all_default(x,y,k):\n",
    "\t\"\"\"\n",
    "\tFits a one-vs-all classification model on a given dataset of k > 2 classes.\n",
    "\tSplits the dataset(x,y) into training and test sets, using the train_test_split function from sklearn. Default test_size = 0.2.\n",
    "\tInputs:\n",
    "\t    x: array\n",
    "\t        input array of input points, without the intercept(raw data).\n",
    "\t    y: array\n",
    "\t        input array of output points, usually a column vector.\n",
    "\tReturns:\n",
    "\t    theta: array\n",
    "\t        outputs array of predictors/parameters calculated by the algorithm.\n",
    "\t    accuracy: float\n",
    "\t        outputs the approximate percentual accuracy of the model, counting each misclassification and calculating the final score.    \n",
    "\t\"\"\"\n",
    "\tif k <= 2:\n",
    "\t\tprint(\"K must be bigger than two\")\n",
    "\t\treturn ValueError \n",
    "\telse:\n",
    "\t\tones = np.ones((np.size(x,0),1))\n",
    "\t\tx = np.hstack((ones,x))\n",
    "\t\tX_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size = 0.2, random_state=5)\n",
    "\t\tprobability_matrix = np.zeros((np.size(X_test,0),k))\n",
    "\t\ttheta = np.zeros((np.size(X_test,1),k))\n",
    "\t\tfor i in range(k):\n",
    "\t\t\tfor w in range(np.size(Y_train,0)):\n",
    "\t\t\t\tif Y_train[w] == i:\n",
    "\t\t\t\t\tY_train[w] = 0\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tY_train[w] = 1\n",
    "\t\t\tparameters = newton_step(X_train,Y_train,15)\n",
    "\t\t\ttheta[:,i] = parameters[:,0]\n",
    "\t\t\tprob = probability_vector(X_test,parameters)\n",
    "\t\t\tprobability_matrix[:,i] = prob[:,0]\n",
    "\t\tresult = np.zeros((np.size(probability_matrix,0),1))\n",
    "\t\tfor n in range(np.size(probability_matrix,0)):\n",
    "\t\t\tresult[n,0] = np.argmax(probability_matrix[n,:])\n",
    "\t\tfor i in range(np.size(result,0)):\n",
    "\t\t\tcounter = 0\n",
    "\t\t\tif np.absolute(result[i,0] - Y_test[i,0]) == 0:\n",
    "\t\t\t\tcounter = counter \n",
    "\t\t\telse:\n",
    "\t\t\t\tcounter = counter + 1\n",
    "\t\taccuracy = np.round(((np.size(Y_test,0) - counter)/np.size(Y_test,0)) * 100)\n",
    "\t\treturn theta, accuracy\n",
    "\n",
    "class classification_fit:\n",
    "\t\"\"\"\n",
    "\tClass of two different types of classification fits for a given dataset.\n",
    "\tExecutes a given model based on user input.\n",
    "\t\tInputs:\n",
    "\t\t\tx: array\n",
    "\t\t\t\tinput array of input points, without the intercept(raw data).\n",
    "\t\t\ty: array\n",
    "\t\t\t\tinput array of output points, usually a column vector.\n",
    "\t\t\tk: int\n",
    "\t\t\t\tinput the number k of classes associated with the dataset \n",
    "\t\tReturns:\n",
    "\t\t\tself.bt: array\n",
    "\t\t\t\toutputs the array of parameters/predictors calculated by the binary model.\n",
    "\t\t\tself.ba: float\n",
    "\t\t\t\toutputs the accuracy of the binary model.\n",
    "\t\t\tself.kt: array \n",
    "\t\t\t\toutputs the array of parameters/predictors calculated by the one-vs-all model.\n",
    "\t\t\tself.ka: float\n",
    "\t\t\t\toutputs the accuracy of the one-vs-all model. \n",
    "\t\"\"\"\n",
    "\tdef __init__(self,x,y,k):\n",
    "\t\tif k == 2:\n",
    "\t\t\tself.bt = binary_classification_default(x,y)[0]\n",
    "\t\t\tself.ba = binary_classification_default(x,y)[1]\n",
    "\t\telse:\n",
    "\t\t\tself.kt = one_vs_all_default(x,y,k)[0]\n",
    "\t\t\tself.ka = one_vs_all_default(x,y,k)[1]\n",
    "#REGRESSION#\n",
    "def regressionQR(x,y):\n",
    "\t\"\"\"\n",
    "\tCalculates the predictors for a linear regression model, using QR decomposition.\n",
    "\tInputs:\n",
    "\t    x: array\n",
    "\t        input array of input points, with the intercept.\n",
    "\t    y: array\n",
    "\t        input array of output points, usually a column vector.\n",
    "\tReturns:\n",
    "\t    theta: array\n",
    "\t        outputs the array of predictors for the regression model.\n",
    "\t\"\"\"\n",
    "\tq, r = np.linalg.qr(x) \n",
    "\tb = q.T @ y\n",
    "\ttheta = linalg.solve_triangular(r,b)\n",
    "\treturn theta\n",
    "def calculate_error(x,y,parameters):\n",
    "\t\"\"\"\n",
    "\tCalculates the squared error of a given model.\n",
    "\tInputs:\n",
    "\t    x: array\n",
    "\t        input array of input points, with the intercept.\n",
    "\t    y: array\n",
    "\t        input array of output points, usually a column vector.\n",
    "\t    parameters: array\n",
    "\t        input the column vector of predictors.\n",
    "\tReturns:\n",
    "\t    errors: array\n",
    "\t        outputs a column vector of squared errors.\n",
    "\t    errors_sum: float\n",
    "\t        outputs the sum of the errors vector.\n",
    "\t\"\"\"\n",
    "\tprediction = (x @ parameters).reshape((np.size(x,0)),1)\n",
    "\terrors = np.square(prediction - y)\n",
    "\terrors_sum = sum(errors)\n",
    "\treturn errors, errors_sum\n",
    "def linear_regression(x,y):\n",
    "\t\"\"\"\n",
    "\tFits a linear regression model on a given dataset.\n",
    "\tInputs:\n",
    "\t    x: array\n",
    "\t        input array of input points, with the intercept.\n",
    "\t    y: array\n",
    "\t        input array of output points, usually a column vector.\n",
    "\tReturns:\n",
    "\t    theta: array\n",
    "\t        outputs the array of predictors for the regression model.\n",
    "\t    MSE: float\n",
    "\t        outputs the approximate Mean Squared Error found by the regression model. MSE is rounded up to 2 decimal cases.\n",
    "\n",
    "\t\"\"\"\n",
    "\tones = np.ones((np.size(x,0),1))\n",
    "\tx = np.hstack((ones,x))\n",
    "\tX_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size = 0.2, random_state=5)\n",
    "\tX_test = X_test.reshape((np.size(X_test,0),np.size(X_train,1)))\n",
    "\tY_test = Y_test.reshape((np.size(Y_test,0),1))\n",
    "\ttheta = regressionQR(X_train,Y_train)\n",
    "\terror = calculate_error(X_test,Y_test,theta)[1]\n",
    "\tMSE = error/np.size(X_test,0)\n",
    "\treturn theta, np.round(MSE,2)\n",
    "def basis_expansion(x,y,btype):\n",
    "\t\"\"\"\n",
    "\tExecutes a basis expansion on the array of inputs and then fits a linear regression model on the dataset.\n",
    "\tInputs:\n",
    "\t    x: array\n",
    "\t        input array of input points, with the intercept.\n",
    "\t    y: array\n",
    "\t        input array of output points, usually a column vector.\n",
    "\t    btype: string\n",
    "\t        input string that identifies the type of basis expansion; btype can be: None, sqrt, poly.\n",
    "\tReturns:\n",
    "\t    theta: array\n",
    "\t        outputs the array of predictors for the regression model.\n",
    "\t    MSE: float\n",
    "\t        outputs the approximate Mean Squared Error found by the regression model. MSE is rounded up to 2 decimal cases.\n",
    "\t\"\"\"\n",
    "\tones = np.ones((np.size(x,0),1))\n",
    "\tx = np.hstack((ones,x))\n",
    "\tif btype == 'sqrt':\n",
    "\t\tX_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size = 0.2, random_state=5)\n",
    "\t\tX_test = X_test.reshape((np.size(X_test,0),np.size(X_train,1)))\n",
    "\t\tY_test = Y_test.reshape((np.size(Y_test,0),1))\n",
    "\t\tfor i in range(np.size(x,1)):\n",
    "\t\t\tX_train[:,i] = np.sqrt(X_train[:,i])\n",
    "\t\t\tX_test[:,i] = np.sqrt(X_test[:,i])\n",
    "\t\ttheta = regressionQR(X_train,Y_train)\n",
    "\t\terror = calculate_error(X_test,Y_test,theta)[1]\n",
    "\t\tMSE = error/np.size(X_test,0)\n",
    "\t\treturn theta,np.round(MSE,2)\n",
    "\tif btype == 'poly':\n",
    "\t\tX_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size = 0.2, random_state=5)\n",
    "\t\tX_test = X_test.reshape((np.size(X_test,0),np.size(X_train,1)))\n",
    "\t\tY_test = Y_test.reshape((np.size(Y_test,0),1))\n",
    "\t\tfor i in range(np.size(x,1)):\n",
    "\t\t\tX_train[:,i] = (X_train[:,i] **(i)) \n",
    "\t\t\tX_test[:,i] = (X_test[:,i] **(i))\n",
    "\t\ttheta = regressionQR(X_train,Y_train)\n",
    "\t\terror = calculate_error(X_test,Y_test,theta)[1]\n",
    "\t\tMSE = error/np.size(X_test,0)\n",
    "\t\treturn theta,np.round(MSE,2)\n",
    "\telse:\n",
    "\t\tprint(\"Insert a valid expansion type\")\n",
    "\t\treturn ValueError \n",
    "def ridge_regression(x,y,const):\n",
    "\t\"\"\"\n",
    "\tFits a ridge linear regression model on a given dataset.\n",
    "\tInputs:\n",
    "\t\tx: array\n",
    "\t\t\tinput array of input points, with the intercept.\n",
    "\t\ty: array\n",
    "\t\t\tinput array of output points, usually a column vector.\n",
    "\t\tconst: float\n",
    "\t\t\tinput the value for the penalizing constante(lambda) used by the ridge algorithm.\n",
    "\tReturns:\n",
    "\t\ttheta: array\n",
    "\t\t\toutputs the array of predictors for the regression model.\n",
    "\t\tMSE: float\n",
    "\t\t\toutputs the approximate Mean Squared Error found by the regression model. MSE is rounded up to 2 decimal cases.\n",
    "\t\"\"\"\n",
    "\tX_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size = 0.2, random_state=5)\n",
    "\tX_test = X_test.reshape((np.size(X_test,0),np.size(X_train,1)))\n",
    "\tY_test = Y_test.reshape((np.size(Y_test,0),1))\n",
    "\tidentity = np.eye(np.size(X_train,1))\n",
    "\ttheta = np.linalg.inv(X_train.T @ X_train + const * identity) @ X_train.T @ Y_train \n",
    "\terror = calculate_error(X_test,Y_test,theta)[1]\n",
    "\tMSE = error/np.size(X_test,0)\n",
    "\treturn theta, np.round(MSE,2)\n",
    "\n",
    "class regression_fit:\n",
    "\t\"\"\"\n",
    "\tClass of three different regression models.\n",
    "\tExecutes a given model based on user input.\n",
    "\tInputs:\n",
    "\t\tx: array\n",
    "\t\t\tinput array of input points, with the intercept.\n",
    "\t\ty: array\n",
    "\t\t\tinput array of output points, usually a column vector.\n",
    "\t\tbtype: string\n",
    "\t\t\tinput string that identifies the type of basis expansion; btype can be: None, sqrt, poly.\n",
    "\tReturns:\n",
    "\t\tself.lt: array\n",
    "\t\t\toutputs the array of parameters/predictors calculated by the standard linear model.\n",
    "\t\tself.le: float\n",
    "\t\t\toutputs the approximate Mean Squared Error calculated by the standard linear model.\n",
    "\t\tself.rt: array\n",
    "\t\t\toutputs the array of parameters/predictors calculated by the ridge linear model.\n",
    "\t\tself.re: float\n",
    "\t\t\toutputs the approximate Mean Squared Error calculated by the ridge linear model.\n",
    "\t\tself.bet: array\n",
    "\t\t\toutputs the array of parameters/predictors calculated by the basis expansion model.\n",
    "\t\tself.bee: float\n",
    "\t\t\toutputs the approximate Mean Squared Error calculated by the basis expansion model.\n",
    "\t\"\"\"\n",
    "\tdef __init__(self,x,y,btype):\n",
    "\t\tif btype == None:\n",
    "\t\t\tself.lt = linear_regression(x,y)[0]\n",
    "\t\t\tself.le = linear_regression(x,y)[1]\n",
    "\t\t\tself.rt = ridge_regression(x,y,0.1)[0]\n",
    "\t\t\tself.re = ridge_regression(x,y,0.1)[1]\n",
    "\t\telse:\n",
    "\t\t\tself.bet = basis_expansion(x,y,btype)[0]\n",
    "\t\t\tself.bee = basis_expansion(x,y,btype)[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we'll see the actual performance of the algorithm for many different famous datasets available at scikitlearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import basicMLp as bp\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Wine recognition dataset, consisting of a classification problem with k = 3 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model is: 97.22%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "X,y = load_wine(return_X_y=True)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X)\n",
    "result = bp.classification_fit(X,y,3)\n",
    "acc = result.ka \n",
    "print(f\"The accuracy of the model is: {acc}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Iris plants dataset, consisting of a classification problem with k = 3 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model is: 96.67%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "X,y = load_iris(return_X_y=True)\n",
    "result = bp.classification_fit(X,y,3)\n",
    "acc = result.ka \n",
    "print(f\"The accuracy of the model is: {acc}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Boston house prices dataset, consisting of a regression problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE of the model is: [16.23]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "X,y = load_boston(return_X_y=True)\n",
    "result = bp.regression_fit(X,y,'poly')\n",
    "acc = result.bee \n",
    "print(f\"The MSE of the model is: {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Wisconsin breast cancer dataset, consisting of a classification problem with k = 2 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model is: 95.61%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "X,y = load_breast_cancer(return_X_y=True)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X)\n",
    "result = bp.classification_fit(X,y,2)\n",
    "acc = result.ba\n",
    "print(f\"The accuracy of the model is: {acc}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
